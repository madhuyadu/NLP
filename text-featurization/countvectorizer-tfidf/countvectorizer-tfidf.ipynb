{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23df8d57",
   "metadata": {},
   "source": [
    "Installing & importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16fefdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (63.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91962\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\91962\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_sm   # download spacy english dictionary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import string  # package used for punctuations\n",
    "import spacy  # package for NLP/text processing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5071e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b903423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print((data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496f82c",
   "metadata": {},
   "source": [
    "Filter out only first 1000 rows & select only 'toxic' & 'comment_text' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c573eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data[:1000][['comment_text', 'toxic']]\n",
    "print(len(data))\n",
    "print((data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92e61205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71e8a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'may', 'has', 'must', 'here', 'an', 'seeming', 'is', 'below', 'why', 'across', 'her', 'upon', 'formerly', 'whatever', 'whenever', 'someone', 'do', 'amount', 'one', 'him', 'therefore', 'along', 'those', 'from', 'around', 'am', 'forty', 'hundred', \"'d\", 'enough', 'since', 'former', 'anything', 'thus', 'empty', 'somehow', 'make', 'unless', 'see', 'although', 'give', 'sometimes', 'does', 'three', 'five', 'together', 'some', 'alone', 'hereupon', 'whose', 'himself', 'which', 'anywhere', 'once', 'now', 'everywhere', 'only', 'also', 'never', 'yourselves', 'being', 'beforehand', 'while', 'sometime', 'otherwise', 'less', 'hence', 'a', 'show', 'others', 'about', 'becomes', 'moreover', 'whither', 'ten', '‘s', 'always', 'if', 'made', 'please', 'doing', 'themselves', 'neither', 'had', 'its', 'down', 'top', 'side', 'several', 'were', 'their', 'they', 'are', 'ca', 'in', 'wherein', 'without', 'mine', 'move', 'back', '‘ve', 'nothing', 'into', 'last', 'out', 'you', '’ll', 'we', 'where', 'over', 'them', 'rather', 'go', 'hers', 'least', 'anyhow', 'above', 'yet', 'anyway', 'would', 'thereafter', 'can', 'fifteen', 'yourself', 'same', 'until', 'cannot', 'eight', 'she', 'up', '’d', 'hereby', 'will', 'no', 'six', 'could', 'n’t', 'thereupon', 'thence', 'becoming', 'when', 'fifty', 'the', 'that', 'eleven', 'herein', 'all', 'whereby', 'on', 'more', 'be', 'among', '‘re', 'sixty', 'both', 'towards', 'afterwards', 'not', 'me', 'call', 'our', 'myself', 'various', 'serious', 'still', 'at', 'too', '‘ll', 'whom', 'nor', 'namely', 'whence', 'was', 'name', 'already', 'wherever', 'his', 'twenty', 'none', 'used', 'mostly', 'very', 'further', 'just', 'ours', 'seems', 'full', 'elsewhere', 'might', 'such', 'another', 'than', '‘d', 'whole', 'often', 'within', 'almost', 'regarding', 'should', 'twelve', 'against', 'and', 'few', 'whereas', 'nowhere', 'part', 'third', \"n't\", 'been', 'seemed', 'have', 'it', 'any', 'with', 'there', \"'ll\", \"'m\", '’s', 'thereby', 'itself', \"'re\", 'ever', 'what', 'n‘t', 'whereafter', 'of', 'therein', 'first', 'even', 'as', 'whoever', 'anyone', '‘m', 'somewhere', 'nobody', 'due', 'your', 'using', 'by', 'front', 'per', 'most', 'for', 'everything', 'other', 'beyond', 'except', 'amongst', 'quite', 'take', 'but', 'so', 'after', 'beside', '’m', 'throughout', 'keep', 'i', 'herself', 'though', 'then', 'hereafter', 'thru', 'onto', 'through', 'indeed', 'nevertheless', 'really', 'much', 'something', 'either', 'two', 'put', 'bottom', '’re', 'because', 'this', 'everyone', 'ourselves', 'seem', 'us', 'he', 'off', 'four', 'again', 'noone', 'under', 'to', 'or', 'did', 'whether', 'many', 'say', 'before', 'meanwhile', 'yours', 'toward', 're', 'else', 'between', 'get', 'during', 'whereupon', 'nine', 'who', 'latter', 'behind', 'however', 'how', 'latterly', 'every', 'perhaps', \"'s\", 'each', 'well', 'via', 'become', 'besides', 'done', \"'ve\", '’ve', 'my', 'next', 'became', 'these'}\n"
     ]
    }
   ],
   "source": [
    "# Load nlp spacy english dictionary & add stop words\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # _sm for small\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words) # view all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b74ef822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# load the punctuations from string package\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4838b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer_sample(sentence):\n",
    "    # Creating token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "    print('The raw text: ')\n",
    "    print('--'*50)\n",
    "    print(doc)\n",
    "    print(type(doc))  # this is a doc object\n",
    "    print('--'*50)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase, strip() to remove white spaces\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "    print('The tokenized text: ')\n",
    "    print(mytokens)\n",
    "    print('--'*50)\n",
    "\n",
    "    # Removing stop words & punctuations from each of the sentences in our dataset\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    print('New tokens post removal of stop words & punctuations... ')\n",
    "    print('--'*50)\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71aa5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw text: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Are you gone nuts?? !! }\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The tokenized text: \n",
      "['be', 'you', 'go', 'nuts', '?', '?', '!', '!', '}']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "New tokens post removal of stop words & punctuations... \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nuts']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the function working for sample sentence\n",
    "\n",
    "sentence = \"Are you gone nuts?? !! }\"\n",
    "spacy_tokenizer_sample(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d494ce",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a84ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(tokenizer = spacy_tokenizer_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fa8d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw text: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "i am eating apple, i like apple\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The tokenized text: \n",
      "['i', 'be', 'eat', 'apple', ',', 'i', 'like', 'apple']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "New tokens post removal of stop words & punctuations... \n",
      "----------------------------------------------------------------------------------------------------\n",
      "The raw text: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "i am playing cricket\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The tokenized text: \n",
      "['i', 'be', 'play', 'cricket']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "New tokens post removal of stop words & punctuations... \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit_transform([\"I am eating apple, I like apple\",\"I am playing cricket\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906439",
   "metadata": {},
   "source": [
    "The numbers inside the array depict the no of times each word has appeared in the sentence/document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3097da17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'cricket', 'eat', 'like', 'play'], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.get_feature_names_out()  # all feature names used/extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad9e5e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat': 2, 'apple': 0, 'like': 3, 'play': 4, 'cricket': 1}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.vocabulary_   # feature extracted along with its index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924233c",
   "metadata": {},
   "source": [
    "### Model with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5baee650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "#     print('The raw text: ')\n",
    "#     print('--'*50)\n",
    "#     print(doc)\n",
    "#     print(type(doc))  # this is a doc object\n",
    "#     print('--'*50)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase, strip() to remove white spaces\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "#     print('The tokenized text: ')\n",
    "#     print(mytokens)\n",
    "#     print('--'*50)\n",
    "\n",
    "    # Removing stop words & punctuations from each of the sentences in our dataset\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "#     print('New tokens post removal of stop words & punctuations... ')\n",
    "#     print('--'*50)\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93464bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 6620)\n",
      "(300, 6620)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['comment_text'] # the features\n",
    "ylabels = data['toxic'] # the labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3,stratify=ylabels)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "count_vector = CountVectorizer(tokenizer = spacy_tokenizer)\n",
    "\n",
    "#Extracting features for train and test data\n",
    "X_train_vectors= count_vector.fit_transform(X_train)\n",
    "X_test_vectors= count_vector.transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train_vectors.shape)  #4062 words/features are created\n",
    "print(X_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1323c8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors.toarray()  # visualizing the matrix/representation created for the text data --its very sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcd964c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vectors,y_train)  # fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1225e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9166666666666666\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 0.21875\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(X_test_vectors)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c1777",
   "metadata": {},
   "source": [
    "### Model with TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6db9ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 6620)\n",
      "(300, 6620)\n"
     ]
    }
   ],
   "source": [
    "# In TFIDF--> term freq means freq of word/term in a document, doc freq means freq of the word in the entire corpus\n",
    "# if a word appears more frequently within document and also in the corpus --> more freq words --> less important (for ex: stop words)\n",
    "# if a word appears more frequently within document and less in the corpus --> less freq or rare words --> more important & more weightage to such words\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "\n",
    "#Extracting features for train and test data\n",
    "X_train_vector= tfidf_vector.fit_transform(X_train)\n",
    "X_test_vector= tfidf_vector.transform(X_test)\n",
    "\n",
    "print(X_train_vector.shape)  #4062 words/features are created\n",
    "print(X_test_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7deabce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8966666666666666\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 0.03125\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vector,y_train)\n",
    "predicted = classifier.predict(X_test_vector)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
